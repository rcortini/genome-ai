{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# will use biophython to deal with biological sequences\n",
    "from Bio import SeqIO, Seq\n",
    "from Bio.Alphabet import IUPAC\n",
    "\n",
    "# and of course the Ruggero-detector\n",
    "import ruggero_detector as rd\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import multiply, Input, Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# our beloved fancy progress bar\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-02-18 What makes a promoter?\n",
    "Using the technology that we developed for the Ruggero-detector, we can try to ask the question: what makes a promoter a promoter? We can feed sequences of DNA to a Ruggero-detector, and then try to use the attention mechanism that we developed earlier to figure out what makes the network shifts its prediction on the sequence.\n",
    "\n",
    "## Getting promoter data\n",
    "\n",
    "### SuRE method\n",
    "\n",
    "The publication by J. van Arensbergen et al (Nature Biotechnology 2017) gives a genome-wide map of positions and their promoter activity.\n",
    "\n",
    "The data file is about 6 Gb in size. It would be unnecessary to load all the data in memory. That's why I wrote a simple C program that runs very fast, which can do the job neatly. We need to make sure that we get random lines from that file, taking care of the fact that we uniformly select lines from promoters and non-promoters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data file\n",
    "fname = '../code/randlines/test.dat'\n",
    "data = pd.read_csv(fname, sep='\\t')\n",
    "chromosomes = data['chr'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to go and fetch the sequences that correspond to the positions in the data set. We will use Biopython to load the human genome and read the information on the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load human genome\n",
    "genome_file = '/mnt/shared/seq/hg19/hg19_unmasked.fasta'\n",
    "genome = SeqIO.index(hg38_genome_file,'fasta', alphabet=IUPAC.unambiguous_dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Biopython is fantastic for extracting sequences of DNA, but it is much less fantastic when it comes to speed. If we want to have the hope to get somewhere, we need to get the data ordered by chromosome, otherwise it's going to take forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary of data points, which correspond to a chromosome\n",
    "data_chr = {chromosome : data.loc[data['chr'] == chromosome] for chromosome in chromosomes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use an iteration over all the chromosomes and fetch the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, I prepare the dictionary that encodes the sequence alphabet\n",
    "seq_to_n = {'A' : 0, 'T' : 1, 'C' : 2, 'G' : 3, 'N' : 4}\n",
    "n_to_seq = {v : k for k, v in seq_to_n.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "targets = []\n",
    "for chromosome in chromosomes :\n",
    "    \n",
    "    print(\"Parsing data of chromosome \" + chromosome)\n",
    "\n",
    "    # get the reads corresponding to the current chromosome\n",
    "    chromosome_data = data_chr[chromosome]\n",
    "    c = genome[chromosome]\n",
    "    \n",
    "    # iterate through the reads\n",
    "    for i, row in enumerate(chromosome_data.iterrows()) :\n",
    "        \n",
    "        # parse the information in the row\n",
    "        r = row[1]\n",
    "        start, end, strand = r['start'], r['end'], r['strand']\n",
    "        myseq = c.seq[start:end]\n",
    "        if strand == '-' :\n",
    "            myseq = myseq.reverse_complement()\n",
    "        myseq = myseq.upper()\n",
    "        \n",
    "        # let's figure out whether the sequence was classified as promoter or not\n",
    "        counts = sum(row[1][5:11])\n",
    "        if counts < 10 :\n",
    "            promoter = 0\n",
    "        else :\n",
    "            promoter = 1\n",
    "        \n",
    "        # encode the sequence in the format that will be comprehensible by the AI\n",
    "        seq_encoded = rd.encode_sentence(myseq, seq_to_n)\n",
    "        sequences.append(seq_encoded)\n",
    "        targets.append(promoter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we're ready to prepare the training and testing data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(sequences)\n",
    "ntrain = int(0.9 * N)\n",
    "ntest = N - ntrain\n",
    "padded_sequences = sequence.pad_sequences(sequences, maxlen=200)\n",
    "train_data = to_categorical(padded_sequences[:ntrain])\n",
    "train_targets = targets[:ntrain]\n",
    "test_data = to_categorical(padded_sequences[ntrain:])\n",
    "test_targets = targets[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we're ready to fire our LSTM and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# LSTM layer\n",
    "lstm_units = 128\n",
    "model.add(LSTM(lstm_units, return_sequences=False, input_shape=(None, len(seq_to_n))))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define checkpointer and fit the model\n",
    "checkpointer = ModelCheckpoint(filepath='../data/promoter-detector.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "model.fit(train_data, train_targets,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, no matter what I try here doesn't give any meaningful results.\n",
    "\n",
    "I think I'd better think of a different strategy when it comes to trying to identify promoter sequences: the fact that I padded the sequences to feed them to the LSTM is not the greatest idea. I might be overlooking entirely the important bits of information from the sequences.\n",
    "\n",
    "## Different sequence management\n",
    "\n",
    "I put the bit of code that extracts the random sequences in a script that works in a two-step way: first step is to extract the random sequences (`randlines`) and the second script fetches the sequences from the genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_fname = '../data/rand_sequences-SuRE.dat'\n",
    "def parse_sequence_file(fname) :\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    with open(in_fname, 'r') as f :\n",
    "        for line in f :\n",
    "            target, sequence = line.strip('\\n').split(' ')\n",
    "            sequences.append(sequence)\n",
    "            targets.append(int(target))\n",
    "    return sequences, targets\n",
    "sequences, targets = parse_sequence_file(in_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of lengths of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = np.array([len(s) for s in sequences])\n",
    "plt.hist(seq_lengths, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from here it's clear that we can actually train a lot of times on sequences that have a lot of data points, located in that bulk between sequence length 500-1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_encoded = []\n",
    "for sequence in sequences :\n",
    "    seq_encoded = rd.encode_sentence(sequence, alpha_to_n=seq_to_n)\n",
    "    seq_encoded = to_categorical(seq_encoded, num_classes=len(seq_to_n))\n",
    "    sequences_encoded.append(seq_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(sequences_encoded, targets, alphabet_size) :\n",
    "    \n",
    "    # get the value of the sequence lengths in an array\n",
    "    seq_lengths = np.array([s.shape[0] for s in sequences_encoded])\n",
    "    max_length = seq_lengths.max()\n",
    "    \n",
    "    # we also set a minimum length of sequence that we want to pass to the\n",
    "    # training\n",
    "    min_length = 100\n",
    "    \n",
    "    # the generator loops forever\n",
    "    seq_length = min_length\n",
    "    while True :\n",
    "        \n",
    "        # this statement allows to loop indefinitely through the same data\n",
    "        if seq_length==max_length :\n",
    "            seq_length = min_length\n",
    "        \n",
    "        # select all the sequences that have that fixed length, provided that there\n",
    "        # are a sufficient amount of data points to take into consideration\n",
    "        seq_mask = seq_lengths==seq_length\n",
    "        nseqs = sum(seq_mask)\n",
    "        if nseqs < 10 :\n",
    "            seq_length += 1\n",
    "            continue\n",
    "\n",
    "        # init the objects that will make the output\n",
    "        idxs = np.where(seq_mask)[0]\n",
    "        x_train = np.zeros((nseqs, seq_length, alphabet_size))\n",
    "        y_train = []\n",
    "        for i, idx in enumerate(idxs) :\n",
    "            seq = sequences[idx]\n",
    "            x_train[i, :, :] = sequences_encoded[idx]\n",
    "            y_train.append(targets[idx])\n",
    "        \n",
    "        # increment the sequence length\n",
    "        seq_length += 1\n",
    "        \n",
    "        # and finally, yield\n",
    "        yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# LSTM layer\n",
    "lstm_units = 32\n",
    "model.add(LSTM(lstm_units, return_sequences=False, input_shape=(None, len(seq_to_n))))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define checkpointer and fit the model\n",
    "checkpointer = ModelCheckpoint(filepath='../data/promoter-detector.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_generator(sequences_encoded, targets, seq_to_n),\n",
    "          steps_per_epoch = 100,\n",
    "          epochs=10,\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It won't train. Let's try with dinucleotides or trinucleotides.\n",
    "\n",
    "## More words\n",
    "\n",
    "I want to try here to see what happens if we include more words in the alphabet that we are feeding to the model. I'll dig back my functions that I wrote when I was working on the AI to distinguish between genomes.\n",
    "\n",
    "It turns out that it's very slow for python to process the strings and convert them to their encoding, so I wrote a small C program that does the job very rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_decode(num, mapping, base=5, length=None) :\n",
    "    N = num\n",
    "    i = 0\n",
    "    seq = ''\n",
    "    while N>0 :\n",
    "        ai = N%base**(i+1)//base**i\n",
    "        N -= ai*base**i\n",
    "        i+=1\n",
    "        seq += mapping[ai]\n",
    "    if length is not None :\n",
    "        seq += mapping[0]*(length-len(seq))\n",
    "    return seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_encode(sequence, mapping, base=5) :\n",
    "    l = len(sequence)\n",
    "    return np.sum([base**(l-i-1)*mapping[sequence[i]] for i in range(l)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_encoded_fname = '../data/rand_sequences-encoded-SuRE.dat'\n",
    "sequences_encoded_cat = []\n",
    "nletters = 3\n",
    "base = len(seq_to_n)\n",
    "alphabet_size = base**nletters\n",
    "with open(sequences_encoded_fname, 'r') as f :\n",
    "    for line in f :\n",
    "        sequence_encoded = list(map(int, line.split()))\n",
    "        sequences_encoded_cat.append(to_categorical(sequence_encoded, num_classes=alphabet_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_encoded_cat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# LSTM layer\n",
    "lstm_units = 32\n",
    "model.add(LSTM(lstm_units, return_sequences=False, input_shape=(None, len(seq_to_n))))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define checkpointer and fit the model\n",
    "checkpointer = ModelCheckpoint(filepath='../data/promoter-detector.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_generator(sequences_encoded_cat, targets, alphabet_size),\n",
    "          steps_per_epoch = 100,\n",
    "          epochs=10,\n",
    "          callbacks=[checkpointer])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "vpython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
