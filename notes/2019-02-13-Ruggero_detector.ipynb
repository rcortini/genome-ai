{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import multiply, Input, Dense, Permute, Reshape, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-02-13 Ruggero detector\n",
    "\n",
    "In this notebook I want to explore the idea that an AI can recognize a specific word in a context, and that we can have a hint of what is going on in its ganglia by applying an attention mechanism to it.\n",
    "\n",
    "The target of this notebook is to build a Ruggero detector. I will create random sequences of letters and will put the word \"Ruggero\" (with noise) inside of them or not. Then I will say to the AI that some of the sequences contain the word and some not, without specifying where the word is. It would be really cool if the AI could tell me that it aroused its attention where the word \"Ruggero\" was found.\n",
    "\n",
    "## Building the data set\n",
    "\n",
    "The first thing I want to do is to create a sequence of random letters, some with \"Ruggero\" and some without.\n",
    "\n",
    "First, let's build an alphabet and an encoding of the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's limit ourselves to only lowercase characters\n",
    "letters = string.ascii_lowercase\n",
    "nletters = len(letters)\n",
    "alpha_to_n = {letters[i] : i for i in range(nletters)}\n",
    "n_to_alpha = {i : letters[i] for i in range(nletters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's build a function that can encode a sequence of letters to a sequence of numbers - given the mapping - and another function that can do the opposite: decode a sequence of numbers to a sequence of letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence_alpha, alpha_to_n) :\n",
    "    return [alpha_to_n[sentence_alpha[i]] for i in range(len(sentence_alpha))]\n",
    "\n",
    "def decode_sentence(sentence_n, n_to_alpha) :\n",
    "    return ''.join([n_to_alpha[sentence_n[i]] for i in range(len(sentence_n))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a function that generate random sequences of letters (sentences), giving the option that they may or not contain a given target. We will also give the option of adding noise to the creation of the sentences that contain the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's then build a function that can generate a random sequence with or\n",
    "# without a target sequence (Ruggero)\n",
    "def generate_sentences(nsentences, sentence_length, alpha_to_n,\n",
    "                       seed = None, with_target = False, target = 'ruggero', noise = None) :\n",
    "    \n",
    "    # get length of the alphabet\n",
    "    nletters = len(alpha_to_n)\n",
    "    \n",
    "    # init random number generator if seed is given\n",
    "    if seed is not None :\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # generate the sentences\n",
    "    sentences = np.random.randint(low=0, high=nletters, size=(nsentences, sentence_length))\n",
    "    \n",
    "    # if the user doesn't require the sentences to include the target, we're done,\n",
    "    # otherwise we have to include the target at some location\n",
    "    if with_target :\n",
    "        \n",
    "        # get the encoding of the target\n",
    "        target_encoding = encode_sentence(target, alpha_to_n)\n",
    "        \n",
    "        # generate a list of integers that will specify at which location in the sentence\n",
    "        # the target will be inserted\n",
    "        target_length = len(target)\n",
    "        target_location = np.random.randint(low=0, high=sentence_length-target_length+1,\n",
    "                                           size=nsentences)\n",
    "        for i, loc in enumerate(target_location) :\n",
    "            sentences[i, loc:loc+target_length] = target_encoding\n",
    "        \n",
    "        # now for the noise part: if user requested noise, then we add it\n",
    "        if noise is not None :\n",
    "            \n",
    "            # first, in this case we generate a list of integers that represent\n",
    "            # the numbers of letters of the target that we want to mutate. This\n",
    "            # will be drawn from a uniform distribution between 0 and noise, where\n",
    "            # noise is passed by the user\n",
    "            mutation_size = np.random.randint(low = 0, high = noise, size=nsentences)\n",
    "            \n",
    "            # we the proceed to the mutation\n",
    "            for i in range(nsentences) :\n",
    "                \n",
    "                # we remember where was the target in this sentence\n",
    "                loc = target_location[i]\n",
    "                \n",
    "                # we get the number of letters to mutate\n",
    "                n = mutation_size[i]\n",
    "                \n",
    "                # we pick n letters at random in the target\n",
    "                mutation_locations = np.random.randint(low=0, high=target_length, size=n)\n",
    "                mutations = np.random.randint(low=0, high=nletters, size=n)\n",
    "                \n",
    "                # we then do the mutation\n",
    "                sentences[i, loc+mutation_locations] = mutations\n",
    "                \n",
    "    # at the end, return the generated sentences\n",
    "    # return sentences, mutation_size\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is to test whether this works or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = generate_sentences(10, 20, alpha_to_n,\n",
    "                                              seed=8958574, with_target = True, noise=4.0)\n",
    "for i in range(len(sentences)) :\n",
    "    s = sentences[i]\n",
    "    print(s, decode_sentence(s, n_to_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we have a system to generate noisy sequences that contain the target or not, we can proceed with the next step, which is to generate the data set for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data, targets) :\n",
    "    \"\"\"\n",
    "    Takes a nsentences x nwords \"data\" array and a nsentences-long \"targets\"\n",
    "    array and performs a random permutation of the order, preserving the correspondence\n",
    "    between the row index of the data and the row index of the targets.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    perm = np.random.choice(N, size=N, replace=False)\n",
    "    return data[perm], targets[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sentence_length, ntrain, nvalid, ntest, alpha_to_n, noise=None) :\n",
    "    N = ntrain + nvalid + ntest\n",
    "    \n",
    "    # generate N sentences without the target\n",
    "    sentences_without = generate_sentences(N, sentence_length, alpha_to_n,\n",
    "                                           with_target = False)\n",
    "    targets_without = np.zeros(N, dtype=np.int32)\n",
    "    \n",
    "    # generate N sentences with the target\n",
    "    sentences_with = generate_sentences(N, sentence_length, alpha_to_n,\n",
    "                                           with_target = True, noise = noise)\n",
    "    targets_with = np.ones(N, dtype=np.int32)\n",
    "    \n",
    "    # now stack everything and shuffle\n",
    "    data = np.vstack((sentences_without, sentences_with))\n",
    "    targets = np.concatenate((targets_without, targets_with))\n",
    "    \n",
    "    # shuffle the data\n",
    "    data, targets = shuffle_data(data, targets)\n",
    "    \n",
    "    # now we partition data and targets into train, valid, and test sets\n",
    "    train_data = data[:2*ntrain, :]\n",
    "    train_targets = targets[:2*ntrain]\n",
    "    valid_data = data[2*ntrain:2*(ntrain+nvalid), :]\n",
    "    valid_targets = targets[2*ntrain:2*(ntrain+nvalid)]\n",
    "    test_data = data[2*(ntrain+nvalid):, :]\n",
    "    test_targets = targets[2*(ntrain+nvalid):]\n",
    "    \n",
    "    # one further step is required, then return\n",
    "    train_data = np.expand_dims(train_data, axis = 2)\n",
    "    valid_data = np.expand_dims(valid_data, axis = 2)\n",
    "    test_data = np.expand_dims(test_data, axis = 2)\n",
    "    return train_data, train_targets,\\\n",
    "           valid_data, valid_targets,\\\n",
    "           test_data, test_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a small data set to see whether this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 10\n",
    "nvalid = 5\n",
    "ntest = 2\n",
    "sentence_length = 20\n",
    "train_data, train_targets,\\\n",
    "valid_data, valid_targets,\\\n",
    "test_data, test_targets = prepare_data(sentence_length, ntrain, nvalid, ntest, alpha_to_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(np.squeeze(valid_data, axis=-1)) :\n",
    "    print(s, decode_sentence(s, n_to_alpha), valid_targets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this works. Now we can generate the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 10000\n",
    "nvalid = 2000\n",
    "ntest = 2000\n",
    "sentence_length = 80\n",
    "train_data, train_targets,\\\n",
    "valid_data, valid_targets,\\\n",
    "test_data, test_targets = prepare_data(sentence_length, ntrain, nvalid, ntest, alpha_to_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready now to build the AI.\n",
    "\n",
    "## Building the AI\n",
    "\n",
    "We now want to build an AI architecture that has an attention mechanism. This bit of code I will get from the nice github repository github.com/philipperemy/keras-attention-mechanism. As explained there, there are two kinds of architectures one can try: applying the attention before and after the LSTM. Let's try both.\n",
    "\n",
    "First, I'll steal the \"attention\" function from that repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs, time_steps):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we will feed to the AI will be in the format of a matrix that has dimensions (sentence_length * nletters). Each of the entries (i,j) of the matrix will be 0 or 1 depending on whether the i-th letter of the sentence is letter j or not. We will prepare the data set in this way by using the \"to_categorical\" function from the Keras utils. It will be convenient to keep the original data in order to print the results along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data to categorical\n",
    "train_data_cat = to_categorical(train_data)\n",
    "valid_data_cat = to_categorical(valid_data)\n",
    "test_data_cat = to_categorical(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention after LSTM\n",
    "\n",
    "In this first block we will try to build an AI with an attention block after the LSTM unit. We will then try to analyze the results by looking into the AI's attention layer and looking at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "inputs = Input(shape=(sentence_length, nletters, ))\n",
    "\n",
    "# LSTM layer\n",
    "lstm_units = 32\n",
    "lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "\n",
    "# attention layer\n",
    "attention_mul = attention_3d_block(lstm_out, sentence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "# output layer\n",
    "output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "\n",
    "# put everything together\n",
    "m_after = Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "# compile\n",
    "m_after.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define checkpointer and fit the model\n",
    "checkpointer = ModelCheckpoint(filepath='../data/ruggero-detector-after.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "m_after.fit(train_data_cat, train_targets,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          validation_data=(valid_data_cat, valid_targets),\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate the model's performance on the test data set\n",
    "score, acc = m_after.evaluate(test_data_cat, test_targets,\n",
    "                            batch_size=32,\n",
    "                            verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the model that has 32 LSTM units almost perfectly is able to discriminate sentences that contain or not contain the target.\n",
    "\n",
    "Now we want to go to the question of how this is done. The following function is taken directly from the github repository mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, layer_name=None):\n",
    "    # fetch the outputs of \n",
    "    if layer_name is None:\n",
    "        # all layers with given name\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        # all layer outputs\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]\n",
    "        \n",
    "    # evaluation functions\n",
    "    inp = model.input\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]\n",
    "    \n",
    "    # return the evaluation functions evaluated on the inputs\n",
    "    return [func([inputs, 1.])[0] for func in funcs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select a single test sequence, get the attention matrix associated to it, and print the target, the model's predictions, and the sequence itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "\n",
    "# get the attention matrix\n",
    "attention_matrix = get_activations(m_after,\n",
    "                                   np.expand_dims(test_data_cat[i], axis=0),\n",
    "                                   layer_name='attention_vec')[0].squeeze().transpose()\n",
    "\n",
    "# print model's prediction and the actual target\n",
    "target = test_targets[i]\n",
    "test_sentence = decode_sentence(test_data[i].squeeze(), n_to_alpha)\n",
    "print(target)\n",
    "print(m_after.predict(np.expand_dims(test_data_cat[i], axis=0)))\n",
    "\n",
    "# plot the activation matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20,10))\n",
    "cax = ax.imshow(attention_matrix, aspect = 'auto', cmap=plt.cm.Oranges)\n",
    "plt.colorbar(cax)\n",
    "plt.xticks(np.arange(sentence_length), test_sentence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how hard I try extracting information from this network, nothing really meaningful comes out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention before LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "inputs = Input(shape=(sentence_length, nletters, ))\n",
    "\n",
    "# attention layer\n",
    "attention_mul = attention_3d_block(inputs, sentence_length)\n",
    "\n",
    "# LSTM layer\n",
    "lstm_units = 32\n",
    "lstm_out = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "\n",
    "# output layer\n",
    "output = Dense(1, activation='sigmoid')(lstm_out)\n",
    "\n",
    "# put everything together\n",
    "m_before = Model(inputs=[inputs], outputs=output)\n",
    "\n",
    "# compile\n",
    "m_before.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../data/ruggero-detector-before.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "m_before.fit(train_data_cat, train_targets,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(valid_data_cat, valid_targets),\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = m_before.evaluate(test_data_cat, test_targets,\n",
    "                            batch_size=32,\n",
    "                            verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 108\n",
    "attention_matrix = get_activations(m_before,\n",
    "                                   np.expand_dims(test_data_cat[i], axis=0),\n",
    "                                   layer_name='attention_vec')[0].squeeze().transpose()\n",
    "target = test_targets[i]\n",
    "test_sentence = decode_sentence(test_data[i].squeeze(), n_to_alpha)\n",
    "print(target)\n",
    "print(m_before.predict(np.expand_dims(test_data_cat[i], axis=0)))\n",
    "fig = plt.figure(figsize = (10,3))\n",
    "plt.bar(np.arange(sentence_length), attention_matrix.mean(axis=0))\n",
    "plt.xticks(np.arange(sentence_length), test_sentence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "So in the end I come to a few conclusions.\n",
    "\n",
    "1. The architecture of putting an attention layer after an LSTM works fantastically well as a Ruggero-detector. However, I found it impossible to extract meaning out of it by inspecting the attention layer.\n",
    "\n",
    "2. The architecture of putting the attention before the LSTM is much less efficient as a Ruggero-detector, and produces even more puzzling results in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "vpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
